Automated Security System Monitoring, Optimization, and Predictive Analytics

Project Overview
This project is designed to provide a comprehensive solution for monitoring, analyzing, and predicting failures in security systems. It automates the monitoring of security devices, such as CCTV cameras, access control systems, and intercoms, and offers predictive insights based on machine learning models. The system performs continuous data collection, real-time monitoring via Grafana, incident reporting, and proactive system maintenance through automation tools.

The project integrates multiple technologies to deliver a robust, scalable, and efficient solution. It is containerized using Docker and deployed using Kubernetes. Additionally, the project features a CI/CD pipeline that ensures smooth, automated updates and deployments.

Technologies and Tools Used
Python: For data collection, analysis, and automation scripts.
PostgreSQL: Used as the primary database for storing system logs and metrics.
aiohttp & asyncio: For asynchronous data collection from security devices.
scikit-learn & Optuna: For building and optimizing machine learning models.
SMOTE: For handling class imbalance in predictive models.
Prometheus & Grafana: For real-time monitoring and alerting.
Docker: For containerizing the application.
Kubernetes (k8s): For deploying and managing the system in a scalable and distributed environment.
Jenkins/GitLab CI: For CI/CD pipelines that automate testing, integration, and deployment of updates.
Plotly Dash: For real-time visualizations.
Crontab & Kubernetes CronJobs: For automating regular tasks like system maintenance and incident reporting.

Project Structure

Core Python Scripts

data_collection.py
Collects real-time data from CCTV, access control, and intercom systems.
Uses asynchronous programming (aiohttp, asyncio) to efficiently collect data from multiple sources.
Stores the collected data in PostgreSQL.

Placeholders:
DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD: Set these to your PostgreSQL database configuration.

External Setup:
PostgreSQL database with necessary tables (cctv_logs, access_control_logs, intercom_logs).

data_analysis_and_root_cause.ipynb
Jupyter Notebook for exploratory data analysis (EDA) and root cause identification.
Analyzes CCTV, access control, and intercom data for patterns and anomalies.

Placeholders: Ensure the database connection parameters match your PostgreSQL setup.

External Setup: Jupyter Notebook environment for running this analysis.

data_preparation_for_ml.py
Prepares data for training machine learning models.
Cleans and preprocesses historical data to create a high-quality dataset.

Placeholders: None. The script assumes it pulls from the cleaned data generated by data_collection.py.

model_training_and_evaluation.ipynb
Jupyter Notebook for training and evaluating machine learning models.
Uses Random Forest and Logistic Regression models, optimized with Optuna and cross-validation.

Placeholders: Ensure dataset paths and parameters (e.g., hyperparameters for models) are correctly set.

model_integration.py
Integrates the trained machine learning models for real-time predictions.
Uses the models to predict system failures and sends alerts if issues are detected.
Saves the predictions back to PostgreSQL for visualization in Grafana.

Placeholders:
DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD: Set these to your PostgreSQL configuration.
SMTP_SERVER, SMTP_PORT, EMAIL_USER, EMAIL_PASSWORD, ALERT_EMAIL_RECIPIENT: Configure the email server and recipient for alerting.

External Setup:
A functional SMTP server for sending email alerts.

system_health_monitor.py
Continuously monitors the health of the security system and sends real-time alerts if performance metrics fall outside acceptable thresholds.

Placeholders:
Same database and email placeholders as model_integration.py.

External Setup: None beyond previous setups.

incident_report.py
Generates reports based on incidents detected by the system.

Placeholders: Same database placeholders as model_integration.py.

External Setup: None beyond previous setups.

grafana_dashboard_setup.py
Sets up Grafana dashboards to visualize system performance, failures, and machine learning predictions.

Placeholders:
GRAFANA_API_TOKEN: Required for Grafana API interaction.

External Setup:
A Grafana instance with the Grafana API enabled.

CI/CD Integration
.gitlab-ci.yml
Defines the CI/CD pipeline for automating testing, integration, and deployment of updates.
Includes stages for building Docker images, running tests, and deploying to Kubernetes.

Tests (in /tests/ directory)

test_data_collection.py
Unit tests for validating the data collection process.

test_data_analysis_and_root_cause.py
Tests for data analysis logic and root cause identification functions.

test_model_training_and_evaluation.py
Tests for the machine learning models, including cross-validation and hyperparameter tuning.

test_model_integration.py
Tests the integration of models with real-time data and predictions.

test_incident_report.py
Ensures that incident reporting works as expected and reports are generated correctly.

test_system_health_monitor.py
Verifies that the health monitoring functions and alerts are triggered as expected.

test_grafana_dashboard_setup.py
Tests the proper setup and configuration of Grafana dashboards via API calls.

Deployment Scripts (in /deployment_scripts/ directory)

build_and_push_docker.sh
Builds the Docker image and pushes it to the Docker registry.

Placeholders:
DOCKER_REPO, IMAGE_TAG: Set these to your Docker repository information.

deploy_to_kubernetes.sh
Deploys the Docker container to a Kubernetes cluster.

Placeholders: KUBECONFIG, NAMESPACE: Kubernetes configuration and namespace settings.

setup_grafana_dashboards.sh
Automates the setup of Grafana dashboards using the Grafana API.

run_cron_jobs.sh
Manually triggers the scheduled maintenance and incident reporting cron jobs.

test_deployment.sh
Runs tests after deployment to ensure the system is functioning properly.

cleanup.sh
Cleans up old Docker images, logs, and temporary files.

initialize_ci_cd_pipeline.sh
Sets up the CI/CD pipeline, ensuring all dependencies and configurations are in place.

Kubernetes (in /kubernetes/ directory)

k8s-deployment.yaml
Defines the Kubernetes deployment configuration for running the Docker containers in a scalable and distributed environment.

Placeholders:
DB_HOST, DB_USER, DB_PASSWORD: Database configuration.
GRAFANA_API_TOKEN: Grafana API token.

cronjob.yaml
Defines Kubernetes CronJobs for scheduling regular maintenance and incident reporting tasks.

Instructions for Use

1. Setting Up the Environment

Step 1: PostgreSQL Setup
Ensure you have PostgreSQL installed and running.
Create the necessary database and tables by running the SQL commands in the data_collection.py script.

Set environment variables for database connection in the scripts:
DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD

Step 2: Grafana Setup
Install Grafana and Prometheus.
Configure Grafana to use PostgreSQL as a data source.
Set up dashboards by running grafana_dashboard_setup.py.

Step 3: Email Configuration
Set up an SMTP server (or use an existing one like Gmail).

Set the following placeholders in the scripts:
SMTP_SERVER, SMTP_PORT, EMAIL_USER, EMAIL_PASSWORD, ALERT_EMAIL_RECIPIENT

Step 4: Docker & Kubernetes
Build and deploy the Docker containers using the scripts in the /deployment_scripts/ directory.
Use Kubernetes to deploy the system via k8s-deployment.yaml.

Step 5: CronJobs
Use cronjob.yaml to schedule regular system maintenance and incident reporting.

2. Running the System

Data Collection
Run data_collection.py to begin collecting real-time data from the security devices.

Data Analysis and Model Training
Use the provided Jupyter Notebooks for analyzing the data and training the machine learning models.

System Health Monitoring
Run system_health_monitor.py to continuously monitor the systemâ€™s performance.

Incident Reporting
Run incident_report.py to generate incident reports based on detected system failures.

3. CI/CD Pipeline Setup

Initialize the CI/CD pipeline by running initialize_ci_cd_pipeline.sh.
Configure the .gitlab-ci.yml file with your environment details.
Push your changes to trigger the pipeline.
